from collections import OrderedDict
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm

import torch
import torch.utils.data as data
import torch.optim as optim
import torch.nn.functional as F

from .vae_model import VAEConvNet, vae_loss

def train(model, train_loader, optimizer, epoch, quiet, grad_clip=None):
    model.train()

    if not quiet:
        pbar = tqdm(total=len(train_loader.dataset))
    losses = OrderedDict()
    for x in train_loader:
        x = x.cuda()
        x_recon, mu, log_std = model(x)
        loss_dict = vae_loss(x, x_recon, mu, log_std)
        optimizer.zero_grad()

        loss_dict['loss'].backward()
        if grad_clip:
            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)
        optimizer.step()

        desc = f'Epoch {epoch}'
        for k, v in loss_dict.items():
            if k not in losses:
                losses[k] = []
            losses[k].append(v.item())
            avg_loss = np.mean(losses[k][-50:])
            desc += f', {k} {avg_loss:.4f}'

        if not quiet:
            pbar.set_description(desc)
            pbar.update(x.shape[0])
    if not quiet:
        pbar.close()
    return losses

def eval_loss(model, data_loader, quiet):
    model.eval()
    total_losses = OrderedDict()
    with torch.no_grad():
        for x in data_loader:
            x = x.cuda()
            x_recon, mu, log_std = model(x)
            loss_dict = vae_loss(x, x_recon, mu, log_std)
            for k, v in loss_dict.items():
                total_losses[k] = total_losses.get(k, 0) + v.item() * x.shape[0]

        desc = 'Test '
        for k in total_losses.keys():
            total_losses[k] /= len(data_loader.dataset)
            desc += f', {k} {total_losses[k]:.4f}'
        if not quiet:
            print(desc)
    return total_losses

def train_epochs(model, train_loader, test_loader, train_args, quiet=False):
    # Implement the training and testing of model. 
    # Record the losses during training and testing
    epochs, lr = train_args['epochs'], train_args['lr']
    grad_clip = train_args.get('grad_clip', None)
    optimizer = optim.Adam(model.parameters(), lr=lr)

    train_losses, test_losses = OrderedDict(), OrderedDict()
    for epoch in range(epochs):
        model.train()
        train_loss = train(model, train_loader, optimizer, epoch, quiet, grad_clip)
        test_loss = eval_loss(model, test_loader, quiet)

        for k in train_loss.keys():
            if k not in train_losses:
                train_losses[k] = []
                test_losses[k] = []
            train_losses[k].extend(train_loss[k])
            test_losses[k].append(test_loss[k])

    return train_losses, test_losses


def train_vae(train_data, test_data): 
    """
    A function completing the VAE model training, testing, sampling.  
    Input: 
    - train_data: a numpy array of image for training
    - test_data: a numpy array of image for testing
    return: 
    - train_losses: a B x 3 numpy array that records the losses in training, 
      where B is the number of mini-batches. 
      train_losses[:, 0] records negative ELBO during training
      train_losses[:, 1] records reconstruction losses during training
      train_losses[:, 2] records KL losses during training
    - test_losses: a M x 3 numpy array that records the losses in training,
      where M is the number of epochs. 
      test_losses[:, 0] records negative ELBO during testing
      test_losses[:, 1] records reconstruction losses during testing
      test_losses[:, 2] records KL losses during testing
    - samples: a numpy array of shape (N, C, H, W), which saves samples generated by the trained model
    - reconstructions: a numpy array of shape (N, C, H, W), which saves original images and reconstructed image pairs 
    - interps: a numpy array of shape (N, C, H, W), which saves interpolation between two latent variables. 
    
    ---------------------------------
    
    To train a model, we'll 
    - Construct a dataset using *torch.utils.data.DataLoader*. 
      Normalize the input to [-1, 1]
      The output of the Dataloader is images without labels. 
    - Construct a VAE model and loss (located in *models/vae_model.py*)
    - Train the VAE model. 
    - Samples from the VAE model. 
    - Get original images and reconstructed image pairs.
    - Get interpolations between different latent variable pairs. 
      To do so, we could first get the latent variables from random images pairs, 
      Then get the interpolation of the latent variables using linear combination, 
      and finally get the output using the decoder of VAE. 
    
    Recommended hyperparameters: 
    - Batch size: 128
    - Learning rate: 0.001
    - Total epochs: 20
    - Adam optimizer 
    """
    model = VAEConvNet((3, 32, 32), 16).cuda()
    train_loader = data.DataLoader(train_data, batch_size=128, shuffle=True)
    test_loader = data.DataLoader(test_data, batch_size=128)
    train_losses, test_losses = train_epochs(model, train_loader, test_loader,
                                             dict(epochs=20, lr=1e-3), quiet=False)
    train_losses = np.stack((train_losses['loss'], train_losses['recon_loss'], train_losses['kl_loss']), axis=1)
    test_losses = np.stack((test_losses['loss'], test_losses['recon_loss'], test_losses['kl_loss']), axis=1)
    samples = model.sample(100) * 255.

    x = next(iter(test_loader))[:50].cuda()
    with torch.no_grad():
        x = 2 * x - 1
        z, _ = model.encoder(x)
        x_recon = torch.clamp(model.decoder(z), -1, 1)
    reconstructions = torch.stack((x, x_recon), dim=1).view(-1, 3, 32, 32) * 0.5 + 0.5
    reconstructions = reconstructions.cpu().numpy() * 255

    x = next(iter(test_loader))[:20].cuda()
    with torch.no_grad():
        x = 2 * x - 1
        z, _ = model.encoder(x)
        z1, z2 = z.chunk(2, dim=0)
        interps = [model.decoder(z1 * (1 - alpha) + z2 * alpha) for alpha in np.linspace(0, 1, 10)]
        interps = torch.stack(interps, dim=1).view(-1, 3, 32, 32)
        interps = torch.clamp(interps, -1, 1) * 0.5 + 0.5
    interps = interps.cpu().numpy() * 255

    return train_losses, test_losses, samples, reconstructions, interps

